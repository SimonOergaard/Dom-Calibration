Training on 20000 events and validating on 4000 events.
[1;34mgraphnet[0m [MainProcess] [32mINFO    [0m 2025-01-21 14:50:50 - IceCubeUpgrade.__init__ - Writing log to [1mlogs/graphnet_20250121-145050.log[0m
/groups/icecube/simon/Icecube/src/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python GNN_regression.py ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
No pre-trained model found. Training new model...
/groups/icecube/simon/Icecube/src/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/groups/icecube/simon/Icecube/src/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "

  | Name              | Type       | Params | Mode
---------------------------------------------------------
0 | _tasks            | ModuleList | 387    | train
1 | _graph_definition | KNNGraph   | 0      | train
2 | backbone          | DynEdge    | 1.4 M  | train
---------------------------------------------------------
1.4 M     Trainable params
0         Non-trainable params
1.4 M     Total params
5.530     Total estimated model params size (MB)
36        Modules in train mode
0         Modules in eval mode
Epoch 0:   0%|                                                                                                                                                               | 0/40 [00:00<?, ?it/s]
/groups/icecube/simon/Icecube/src/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

Detected KeyboardInterrupt, attempting graceful shutdown ...
